import streamlit as st
import google.generativeai as genai
from duckduckgo_search import DDGS
from datetime import datetime

# 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© Ø§Ù„ÙØ§Ø¦Ù‚Ø©
st.set_page_config(page_title="Jo Ai 2.0 - Thinking", page_icon="ğŸ§ ", layout="centered")

# ØªØµÙ…ÙŠÙ… ÙˆØ§Ø¬Ù‡Ø© Ø§Ø­ØªØ±Ø§ÙÙŠØ© Ø¯Ø§ÙƒÙ†Ø© (Dark Mode)
st.markdown("""
<style>
    * { direction: rtl; text-align: right; }
    .stApp { background: #0e1117; color: white; }
    .stChatMessage { border-radius: 15px; border: 1px solid #30363d; margin-bottom: 10px; }
</style>
""", unsafe_allow_html=True)

st.markdown('<div style="text-align:center;"><h1>ğŸ§  Ø¬Ùˆ Ø¢ÙŠ 2.0 - Thinking</h1><p>Ø£Ø°ÙƒÙ‰ Ù†Ø³Ø®Ø© Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù… Ø­Ø§Ù„ÙŠØ§Ù‹</p></div>', unsafe_allow_html=True)

# 2. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Gemini 2.0
if "GOOGLE_API_KEY" in st.secrets:
    try:
        genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ø³Ø®Ø© Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù…ØªØ·ÙˆØ± gemini-2.0-flash-thinking-exp
        model = genai.GenerativeModel('gemini-2.0-flash-thinking-exp-01-21')
    except Exception as e:
        st.error("ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù„Ø£ÙˆÙ„ÙŠØŒ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰...")
else:
    st.warning("âš ï¸ ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ GOOGLE_API_KEY ÙÙŠ Secrets")

# 3. Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# 4. Ø§Ù„ØªÙØ§Ø¹Ù„
user_input = st.chat_input("Ø§Ø³Ø£Ù„ Ø£Ø°ÙƒÙ‰ Ù†Ø³Ø®Ø© Ù…ÙˆØ¬ÙˆØ¯Ø©...")

if user_input:
    with st.chat_message("user"):
        st.markdown(user_input)
    st.session_state.messages.append({"role": "user", "content": user_input})
    
    with st.spinner("â³ Ø§Ù„Ù†Ø³Ø®Ø© 2.0 Ø¹Ù… Ø¨ØªÙÙƒØ± Ø¨Ø¹Ù…Ù‚..."):
        try:
            # Ø§Ù„Ø¨Ø­Ø« Ù„Ø¯Ø¹Ù… "Ø§Ù„ØªÙÙƒÙŠØ±"
            search_context = ""
            try:
                with DDGS() as ddgs:
                    search_context = str([r for r in ddgs.text(user_input, max_results=1)])
            except: pass

            prompt = f"Ø£Ù†Øª Jo Ai 2.0ØŒ ØªØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ© Thinking. Ø£Ø¬Ø¨ Ø¨Ù„Ù‡Ø¬Ø© Ø£Ø±Ø¯Ù†ÙŠØ© Ø°ÙƒÙŠØ© Ø¬Ø¯Ø§Ù‹. Ø§Ù„Ø³Ø¤Ø§Ù„: {user_input}\nÙ…Ø¹Ù„ÙˆÙ…Ø§Øª: {search_context}"
            response = model.generate_content(prompt)
            
            with st.chat_message("assistant"):
                st.markdown(response.text)
            st.session_state.messages.append({"role": "assistant", "content": response.text})
            
        except Exception as e:
            # Ø­Ù„ Ø°ÙƒÙŠ: Ø¥Ø°Ø§ ÙØ´Ù„ 2.0ØŒ ÙŠØ­ÙˆÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ù„Ù€ 1.5 Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø±Ø¯
            try:
                fallback_model = genai.GenerativeModel('gemini-1.5-flash')
                response = fallback_model.generate_content(user_input)
                st.session_state.messages.append({"role": "assistant", "content": response.text})
                st.rerun()
            except:
                st.error("Ø§Ù„Ø¶ØºØ· Ø¹Ø§Ù„ÙŠ Ø¬Ø¯Ø§Ù‹ Ø¹Ù„Ù‰ Ø³ÙŠØ±ÙØ±Ø§Øª Ù‚ÙˆÙ‚Ù„ØŒ Ø¬Ø±Ø¨ Ù…Ø±Ø© Ø«Ø§Ù†ÙŠØ© Ø¨Ø¹Ø¯ 5 Ø«ÙˆØ§Ù†ÙŠ.")
