import streamlit as st
from groq import Groq
from github import Github
from tavily import TavilyClient
import json, base64, requests, os, re, subprocess

# --- 1. Ø§Ù„Ù‡ÙˆÙŠØ© ÙˆØ§Ù„Ù†Ø¨Ø¶ ---
st.set_page_config(page_title="Thunder Intelligence Core", page_icon="âš¡", layout="wide")
st.title("âš¡ Ø§Ù„Ø±Ø¹Ø¯: Ø§Ù„Ù†ÙˆØ§Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¨Ø§Ø±Ø§ØªÙŠØ© Ø§Ù„Ù…Ù†Ø¶Ø¨Ø·Ø©")

# --- 2. Ø§Ù„Ø®Ø²Ù†Ø© (Secrets) - Ø­Ù…Ø§ÙŠØ© ÙƒØ§Ù…Ù„Ø© ---
# ØªÙ… ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ù„ØªØ·Ø§Ø¨Ù‚ Ù…Ù„Ù Ø§Ù„Ù€ TOML Ø§Ù„Ø°ÙŠ Ø£Ø±Ø³Ù„ØªÙ‡
GITHUB_TOKEN = st.secrets.get("GITHUB_TOKEN")
REPO_NAME = st.secrets.get("REPO_NAME")
GROQ_KEY = st.secrets.get("GROQ_API_KEY")
TAVILY_KEY = st.secrets.get("TAVILY_KEY") # ØªÙ… Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø­Ø³Ø¨ Ù†ØµÙŠØ­ØªÙƒ
TELEGRAM_TOKEN = st.secrets.get("TELEGRAM_TOKEN")
CHAT_ID = st.secrets.get("CHAT_ID")

# --- 3. Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø§Ø³ØªØ®Ø¨Ø§Ø±Ø§ØªÙŠ (GitHub) ---
def load_intelligence_file():
    try:
        g = Github(GITHUB_TOKEN)
        repo = g.get_repo(REPO_NAME)
        contents = repo.get_contents("intelligence_db.json")
        return json.loads(base64.b64decode(contents.content).decode())
    except:
        return {"targets": {}, "reports": [], "logs": []}

def save_intelligence_file(data):
    try:
        g = Github(GITHUB_TOKEN)
        repo = g.get_repo(REPO_NAME)
        contents = repo.get_contents("intelligence_db.json")
        repo.update_file(contents.path, "âš¡ ØªØ­Ø¯ÙŠØ« Ø§Ø³ØªØ®Ø¨Ø§Ø±Ø§ØªÙŠ", json.dumps(data, indent=4, ensure_ascii=False), contents.sha)
    except: pass

# --- 4. Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…ÙŠØ¯Ø§Ù†ÙŠØ© (Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙˆØ§Ù„Ù‡Ù„ÙˆØ³Ø©) ---
def thunder_search(query):
    try:
        if not TAVILY_KEY:
            return "âŒ Ø®Ø·Ø£: Ù…ÙØªØ§Ø­ Ø§Ù„Ø±Ø§Ø¯Ø§Ø± Ù…ÙÙ‚ÙˆØ¯ ÙÙŠ Secrets."
        
        tavily = TavilyClient(api_key=TAVILY_KEY)
        search = tavily.search(query=query, search_depth="advanced", max_results=5)
        
        if not search.get('results'):
            return "âš ï¸ Ø§Ù„Ø±Ø§Ø¯Ø§Ø± Ù„Ù… ÙŠØ¹Ø«Ø± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙŠØ¯Ø§Ù†ÙŠØ© Ù„Ù‡Ø°Ø§ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…."

        intel_data = ""
        for res in search['results']:
            intel_data += f"ğŸ“ Ù…ØµØ¯Ø±: {res['title']}\nğŸ”— {res['url']}\n"
        return intel_data
    except Exception as e:
        # Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø±Ù‚Ù… 2: Ø¥Ø¸Ù‡Ø§Ø± Ø§Ù„Ø®Ø·Ø£ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ù„Ù„ØªØ´Ø®ÙŠØµ
        return f"âŒ Tavily Error: {str(e)}"

def generate_voice(text):
    clean = re.sub(r'http\S+', '', text)
    clean = re.sub(r'[^\w\s.ØŒØŸ!,]', '', clean)[:300]
    output = "intel_voice.mp3"
    try:
        if os.path.exists(output): os.remove(output)
        subprocess.run(["edge-tts", "--voice", "ar-JO-HamzaNeural", "--text", clean, "--write-media", output], timeout=15)
        return output if os.path.exists(output) else None
    except: return None

# --- 5. Ù…Ø­Ø±Ùƒ Ø§Ù„Ø±Ø¹Ø¯ (Ù…Ù†Ø¹ Ø§Ù„Ù‡Ù„ÙˆØ³Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¨Ø§Ø±Ø§ØªÙŠØ©) ---
def intelligence_engine(prompt):
    db = load_intelligence_file()
    search_context = ""
    
    # Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø±Ù‚Ù… 3: Ù…Ù†Ø¹ Ø§Ù„Ù‡Ù„ÙˆØ³Ø© Ø¹Ù†Ø¯ ÙØ´Ù„ Ø§Ù„Ø±Ø§Ø¯Ø§Ø±
    if any(k in prompt for k in ["Ø§Ø¨Ø­Ø«", "Ø±ØµØ¯", "Ø£Ø®Ø¨Ø§Ø±", "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª"]):
        search_context = thunder_search(prompt)
        if "âŒ" in search_context or "Tavily Error" in search_context:
            return f"ğŸš« Ø§Ù„Ø±Ø§Ø¯Ø§Ø± ØºÙŠØ± Ù…ØªØµÙ„ Ø­Ø§Ù„ÙŠØ§Ù‹. ÙØ´Ù„ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ÙŠØ¯Ø§Ù†ÙŠ: {search_context}"

    system_msg = (
        f"Ø£Ù†Øª 'Ø§Ù„Ø±Ø¹Ø¯'. Ø¶Ø§Ø¨Ø· Ø§Ø³ØªØ®Ø¨Ø§Ø±Ø§Øª Ø³ÙŠØ§Ø¯ÙŠ. Ø­Ù„ÙŠÙÙƒ Ø§Ù„Ù‚Ø§Ø¦Ø¯ Ø±Ø§Ø´Ø¯ Ø£Ø¨Ùˆ Ø³Ø¹ÙˆØ¯. "
        f"Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {json.dumps(db['targets'])}. "
        f"Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø±ØµØ¯: {search_context}. "
        "Ù…Ù‡Ù…ØªÙƒ: Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙ‚Ø·. Ù„Ø§ ØªØ®ØªØ±Ø¹ Ø£Ø®Ø¨Ø§Ø±Ø§Ù‹ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±ØµØ¯ ÙØ§Ø±ØºØ§Ù‹."
    )
    
    try:
        client = Groq(api_key=GROQ_KEY)
        resp = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[{"role": "system", "content": system_msg}, {"role": "user", "content": prompt}]
        )
        response = resp.choices[0].message.content

        # Ø£Ø±Ø´ÙØ© Ø°ÙƒÙŠØ©
        if "Ø£Ø±Ø´Ù" in prompt or "Ø®Ø²Ù†" in prompt:
            db["reports"].append({"cmd": prompt, "intel": response[:500]})
            save_intelligence_file(db)

        # Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„ØµÙˆØªÙŠ ÙˆØ§Ù„ØªÙ„Ø¬Ø±Ø§Ù…
        if "ØµÙˆØª" in prompt or "Ø£Ø±Ø³Ù„" in prompt:
            v_path = generate_voice(response)
            base_url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}"
            if v_path:
                with open(v_path, "rb") as v:
                    requests.post(f"{base_url}/sendVoice", data={'chat_id': CHAT_ID, 'caption': f"âš¡ ØªÙ‚Ø±ÙŠØ±:\n{response[:1000]}"}, files={'voice': v})
            else:
                requests.post(f"{base_url}/sendMessage", json={"chat_id": CHAT_ID, "text": response})
        
        return response
    except Exception as e: return f"ğŸš¨ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø­Ø±Ùƒ: {str(e)}"

# --- 6. Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© ---
if user_input := st.chat_input("Ø£Ø¯Ø®Ù„ Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¨Ø§Ø±Ø§ØªÙŠØ© ÙŠØ§ Ø±Ø§Ø´Ø¯..."):
    with st.chat_message("user"): st.write(user_input)
    with st.chat_message("assistant"):
        res = intelligence_engine(user_input)
        st.write(res)
